{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a37ba2-37ae-48ce-8f18-b7e4257a251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec23f419-a1f2-4198-b66b-f77953493416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import Flash\n",
    "# import Flash.inference\n",
    "# import Flash.predictor\n",
    "# import Flash.model.model\n",
    "\n",
    "# importlib.reload(Flash)\n",
    "# importlib.reload(Flash.inference)\n",
    "# importlib.reload(Flash.model.model)\n",
    "# importlib.reload(Flash.predictor)\n",
    "\n",
    "from Flash.config import ModelTestConfig\n",
    "from Flash.fn_utils import create_tokenizer, decode_sequence\n",
    "from Flash.data import Data\n",
    "from Flash.predictor import Predictor, sequence_accuracy\n",
    "\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e6849c8-bf74-4eea-9949-1ee2f88ef431",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelTestConfig(\n",
    "    model_name=\"vanilla_64x2_2-2-3-4\",\n",
    "    root_dir=\"/pscratch/sd/r/ritesh11/SYMBA_arxiv/models/QCD\",\n",
    "    data_dir=\"/pscratch/sd/r/ritesh11/SYMBA_arxiv/data/EW/EW_2-2-3_termwise\",\n",
    "    test_batch_size=64,\n",
    "    device=\"cuda\",\n",
    "    embedding_size=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers= 3, \n",
    "    use_torch_mha =False,\n",
    "    kan_ff_dims=[],  # Not using KAN here\n",
    "    is_kan=False,\n",
    "    is_pre_norm=False, \n",
    "    ff_dims=4096,\n",
    "    dropout=0,\n",
    "    src_max_len=3110,\n",
    "    tgt_max_len=2008,\n",
    "    is_termwise=True,\n",
    "    seed=42,\n",
    "    truncate=False,\n",
    "    debug=False,\n",
    "    to_replace=False,\n",
    "    is_beamsearch=False,\n",
    "    beam_width=1,\n",
    "    index_pool_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e65503-476f-4d3e-a14a-1d8bb73b6476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source vocab: 100%|██████████| 681599/681599 [02:28<00:00, 4598.36it/s]\n",
      "Processing target vocab: 100%|██████████| 681599/681599 [01:29<00:00, 7622.53it/s] \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.data_dir + \"train.csv\", low_memory=False)\n",
    "df_test = pd.read_csv(config.data_dir + \"test.csv\")\n",
    "df_valid = pd.read_csv(config.data_dir + \"valid.csv\")\n",
    "\n",
    "# df_train = df_train.iloc[:100]\n",
    "df = pd.concat([df_train, df_valid, df_test]).reset_index(drop=True)\n",
    "\n",
    "tokenizer, src_vocab, tgt_vocab = create_tokenizer(df,config)\n",
    "del df, df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6c1094-afd5-4fd0-b85d-cc1783d54c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_voc_size = len(src_vocab)\n",
    "config.tgt_voc_size = len(tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbafff25-7d0e-4c7f-a9b6-af9c1b352293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.src_voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c634ca-1c3f-429f-ad9f-bec4955a6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(config.data_dir + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c951ff-6c65-476b-a3ff-5858967ff97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81079/81079 [00:10<00:00, 8047.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "df_test[\"length\"] = df_test[\"sqamp\"].progress_apply(lambda x: len(tokenizer.tgt_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511d889a-5aad-4b08-8327-f457808be7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df_test.sort_values(\"length\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f6b70f-4e72-44fd-aafa-2839312d30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Data(df_test,tokenizer,config,src_vocab,tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d288c08f-f0cd-4e35-b3dd-8e249455ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.test_batch_size = 64\n",
    "config.test_size = len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4ce01a-5cef-4f01-80af-fc3f189bf186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:48:50 - INFO - Flash.fn_utils - Weights initialized\n",
      "2025-11-13 01:48:50 - INFO - Flash.fn_utils - Model(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): FlashMHA(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=4096, bias=True)\n",
      "          (linear2): Linear(in_features=4096, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0, inplace=False)\n",
      "          (dropout2): Dropout(p=0, inplace=False)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): FlashMHA(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (cross_attn): FlashMHA(\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=4096, bias=True)\n",
      "          (linear2): Linear(in_features=4096, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0, inplace=False)\n",
      "          (dropout2): Dropout(p=0, inplace=False)\n",
      "          (dropout3): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=187, bias=True)\n",
      "  (src_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(310, 512)\n",
      "  )\n",
      "  (tgt_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(187, 512)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using epoch 34 model for predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seq_Acc_Cal:   1%|          | 2/194 [00:36<59:08, 18.48s/it, seq_accuracy=0.797]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43msequence_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_incorrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2025/Flash/predictor.py:131\u001b[0m, in \u001b[0;36msequence_accuracy\u001b[0;34m(config, test_ds, vocab, load_best, epoch, return_incorrect)\u001b[0m\n\u001b[1;32m    129\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeq_Acc_Cal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_start, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m--> 131\u001b[0m     raw_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, (gt_tokens, pred_tokens) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(raw_pairs):\n\u001b[1;32m    134\u001b[0m         gt \u001b[38;5;241m=\u001b[39m decode_sequence(gt_tokens\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist(), vocab)\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2025/Flash/predictor.py:93\u001b[0m, in \u001b[0;36mPredictor.predict_batch\u001b[0;34m(self, batch, vocab, raw_tokens)\u001b[0m\n\u001b[1;32m     85\u001b[0m         tgt_tokens \u001b[38;5;241m=\u001b[39m beam_search_decode(\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len,\n\u001b[1;32m     87\u001b[0m             src, src_padding_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m             beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeam_width\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m         tgt_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_symbols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_symbols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_tokens:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(gt, pred) \u001b[38;5;28;01mfor\u001b[39;00m gt, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(original_tokens, tgt_tokens)]\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2025/Flash/inference.py:63\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[0;34m(model, device, max_len, src, src_padding_mask, start_symbols, dtype, temperature, top_k)\u001b[0m\n\u001b[1;32m     60\u001b[0m current_memory \u001b[38;5;241m=\u001b[39m memory[alive_idx]\n\u001b[1;32m     61\u001b[0m current_src_mask \u001b[38;5;241m=\u001b[39m src_padding_mask[alive_idx]\n\u001b[0;32m---> 63\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_ys_alive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_src_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerator(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# (alive, vocab)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2025/Flash/model/model.py:565\u001b[0m, in \u001b[0;36mModel.decode\u001b[0;34m(self, tgt, memory, tgt_padding_mask, memory_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    563\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_tok_emb(tgt))\n\u001b[1;32m    564\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkan_embed(tgt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_kan_embed \u001b[38;5;28;01melse\u001b[39;00m tgt\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2025/Flash/model/model.py:340\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_padding_mask, memory_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    338\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 340\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2025/Flash/model/model.py:294\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_padding_mask, memory_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_padding_mask, tgt_is_causal))\n\u001b[0;32m--> 294\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cross_attn_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# x = self.norm3(self._ff_block(x)) if self.is_kan else self.norm3(x + self._ff_block(x))\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_kan \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2025/Flash/model/model.py:258\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._cross_attn_block\u001b[0;34m(self, x, memory, padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cross_attn_block\u001b[39m(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    253\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    257\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 258\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cross\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc9/pytorch/2.6.0/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2025/Flash/model/model.py:109\u001b[0m, in \u001b[0;36mFlashMHA.forward\u001b[0;34m(self, q, k, v, padding_mask, is_cross, causal)\u001b[0m\n\u001b[1;32m    106\u001b[0m q_packed, q_indices, cu_q, max_q, _ \u001b[38;5;241m=\u001b[39m unpad_input(q_proj, q_mask)\n\u001b[1;32m    108\u001b[0m kv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([k_proj, v_proj], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m kv, _, cu_k, max_k, _ \u001b[38;5;241m=\u001b[39m \u001b[43munpad_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m q_packed \u001b[38;5;241m=\u001b[39m q_packed\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    112\u001b[0m kv \u001b[38;5;241m=\u001b[39m kv\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/.local/perlmutter/pytorch2.6.0/lib/python3.12/site-packages/flash_attn/bert_padding.py:116\u001b[0m, in \u001b[0;36munpad_input\u001b[0;34m(hidden_states, attention_mask, unused_mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(all_masks\u001b[38;5;241m.\u001b[39mflatten(), as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    115\u001b[0m max_seqlen_in_batch \u001b[38;5;241m=\u001b[39m seqlens_in_batch\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 116\u001b[0m cu_seqlens \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqlens_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# TD [2022-03-04] We don't want to index with a bool mask, because Pytorch will expand the\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# bool mask, then call nonzero to get the indices, then index with those. The indices is @dim\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# times larger than it needs to be, wasting memory. It's faster and more memory-efficient to\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# index with integer indices. Moreover, torch's index is a bit slower than it needs to be,\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# so we write custom forward and backward to make it a bit faster.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m     index_first_axis(rearrange(hidden_states, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s ... -> (b s) ...\u001b[39m\u001b[38;5;124m\"\u001b[39m), indices),\n\u001b[1;32m    124\u001b[0m     indices,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     used_seqlens_in_batch, \n\u001b[1;32m    128\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    res = sequence_accuracy(config, test_ds, tgt_vocab, return_incorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee724656-f0dc-45c7-860e-9cd53b2a17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_idxs = res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90697e41-4f44-48a7-b01d-3bc1f753048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_arr = np.array(inc_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f7e5ae7-959b-4863-8df7-cee16d5938fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"QCD_2-3.npy\",inc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a540961-42e9-4c4b-9f04-aabff9f7e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_idxs = np.load(\"EW_2-2-3-vanilla_inc.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb8066b5-326e-4627-a901-1530cbcb552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_set = set(df_sorted.iloc[inc_idxs].amp.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ddd003-1b66-4da6-ac5f-318983aea3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5117"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77072cad-465d-440f-a913-3abfa3934ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_set = set(df_sorted.amp.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e675a09c-2283-4f49-8a44-06dc98bf5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds = len(complete_set - inc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f7b9e7-863b-4e9d-bdd4-d2dacb383f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of correctly predicted amplitudes\n",
    "corr_set = complete_set - inc_set  \n",
    "\n",
    "# keep only rows where amp is in corr_set\n",
    "df_correct = df_sorted[df_sorted[\"amp\"].isin(corr_set)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e931fafd-0e3b-443d-85ea-5d554ac42bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_unique = df_correct.drop_duplicates(subset=[\"amp\"], keep=\"first\")\n",
    "df_unique = df_test.drop_duplicates(subset=['amp'], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0ba4245-1b3d-4c94-93f1-c2f97e7741d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835645917646303"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corr_set) / len(complete_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40caf04a-12e4-41cf-942b-e086dcf5d068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amp</th>\n",
       "      <th>sqamp</th>\n",
       "      <th>process</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1/2*i*e^2*sin(theta_W)^(-2)*P_L_{ % INDEX_0, ...</td>\n",
       "      <td>&lt;BOS&gt;e^4*MOMENTUM_12*MOMENTUM_34*(m_W^2+-m_d^2...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/12*i*e*sin(theta_W)*(e*sin(theta_W)/cos(thet...</td>\n",
       "      <td>&lt;BOS&gt;1/36*e^2*MOMENTUM_13*MOMENTUM_24*(m_u^2+-...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/3*i*e^2*P_L_{ % INDEX_0, % INDEX_1}*gamma_{+...</td>\n",
       "      <td>&lt;BOS&gt;-4/9*e^4*(m_e^2*MOMENTUM_13+-MOMENTUM_14*...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/9*i*e^2*P_L_{ % INDEX_0, % INDEX_1}*P_L_{ % ...</td>\n",
       "      <td>&lt;BOS&gt;4/81*e^4*MOMENTUM_14*MOMENTUM_23*(m_u^2+-...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1/18*i*e*sin(theta_W)*(e*sin(theta_W)/cos(the...</td>\n",
       "      <td>&lt;BOS&gt;1/81*e^2*MOMENTUM_12*MOMENTUM_34*(m_u^2+-...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81069</th>\n",
       "      <td>1/4*i*e*v^(-2)*m_e*m_u*(P_L_{ % INDEX_0, % IND...</td>\n",
       "      <td>&lt;BOS&gt;1/2*e^2*v^(-4)*m_e^2*m_u^2*MOMENTUM_12*MO...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81071</th>\n",
       "      <td>-1/54*i*e^2*sin(theta_W)*(e*sin(theta_W)/cos(t...</td>\n",
       "      <td>&lt;BOS&gt;4/729*e^4*MOMENTUM_13*MOMENTUM_14*MOMENTU...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81073</th>\n",
       "      <td>1/27*i*e^3*cos(theta_W)^(-2)*sin(theta_W)^2*(P...</td>\n",
       "      <td>&lt;BOS&gt;16/729*e^6*MOMENTUM_13*MOMENTUM_15*MOMENT...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81075</th>\n",
       "      <td>-1/54*i*e^2*sin(theta_W)*(e*sin(theta_W)/cos(t...</td>\n",
       "      <td>&lt;BOS&gt;4/729*e^4*MOMENTUM_13*MOMENTUM_14*MOMENTU...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81077</th>\n",
       "      <td>1/27*i*e^3*cos(theta_W)^(-2)*sin(theta_W)^2*(P...</td>\n",
       "      <td>&lt;BOS&gt;16/729*e^6*MOMENTUM_13*MOMENTUM_15*MOMENT...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31134 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     amp  \\\n",
       "0      -1/2*i*e^2*sin(theta_W)^(-2)*P_L_{ % INDEX_0, ...   \n",
       "1      1/12*i*e*sin(theta_W)*(e*sin(theta_W)/cos(thet...   \n",
       "2      1/3*i*e^2*P_L_{ % INDEX_0, % INDEX_1}*gamma_{+...   \n",
       "3      1/9*i*e^2*P_L_{ % INDEX_0, % INDEX_1}*P_L_{ % ...   \n",
       "4      -1/18*i*e*sin(theta_W)*(e*sin(theta_W)/cos(the...   \n",
       "...                                                  ...   \n",
       "81069  1/4*i*e*v^(-2)*m_e*m_u*(P_L_{ % INDEX_0, % IND...   \n",
       "81071  -1/54*i*e^2*sin(theta_W)*(e*sin(theta_W)/cos(t...   \n",
       "81073  1/27*i*e^3*cos(theta_W)^(-2)*sin(theta_W)^2*(P...   \n",
       "81075  -1/54*i*e^2*sin(theta_W)*(e*sin(theta_W)/cos(t...   \n",
       "81077  1/27*i*e^3*cos(theta_W)^(-2)*sin(theta_W)^2*(P...   \n",
       "\n",
       "                                                   sqamp process  length  \n",
       "0      <BOS>e^4*MOMENTUM_12*MOMENTUM_34*(m_W^2+-m_d^2...     2-2      44  \n",
       "1      <BOS>1/36*e^2*MOMENTUM_13*MOMENTUM_24*(m_u^2+-...     2-2      73  \n",
       "2      <BOS>-4/9*e^4*(m_e^2*MOMENTUM_13+-MOMENTUM_14*...     2-2      48  \n",
       "3      <BOS>4/81*e^4*MOMENTUM_14*MOMENTUM_23*(m_u^2+-...     2-2      33  \n",
       "4      <BOS>1/81*e^2*MOMENTUM_12*MOMENTUM_34*(m_u^2+-...     2-2      74  \n",
       "...                                                  ...     ...     ...  \n",
       "81069  <BOS>1/2*e^2*v^(-4)*m_e^2*m_u^2*MOMENTUM_12*MO...     2-3      79  \n",
       "81071  <BOS>4/729*e^4*MOMENTUM_13*MOMENTUM_14*MOMENTU...     2-3      94  \n",
       "81073  <BOS>16/729*e^6*MOMENTUM_13*MOMENTUM_15*MOMENT...     2-3      76  \n",
       "81075  <BOS>4/729*e^4*MOMENTUM_13*MOMENTUM_14*MOMENTU...     2-3      94  \n",
       "81077  <BOS>16/729*e^6*MOMENTUM_13*MOMENTUM_15*MOMENT...     2-3      76  \n",
       "\n",
       "[31134 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e37ad3cb-d45a-4c31-9d93-858a28915562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amp</th>\n",
       "      <th>sqamp</th>\n",
       "      <th>process</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1/2*i*e^2*(P_L_{ % INDEX_0, % INDEX_1}*MOMENT...</td>\n",
       "      <td>&lt;BOS&gt;e^4*MOMENTUM_13*MOMENTUM_34*(MOMENTUM_12+...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1/2*i*e^2*(P_L_{ % INDEX_0, % INDEX_1}*MOMENT...</td>\n",
       "      <td>&lt;BOS&gt;e^4*MOMENTUM_13*MOMENTUM_34*(MOMENTUM_12+...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1/2*i*e^2*(P_R_{ % INDEX_0, % INDEX_1}*MOMENT...</td>\n",
       "      <td>&lt;T1&gt;e^4*MOMENTUM_23*MOMENTUM_24*(MOMENTUM_23+-...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1/2*i*e^2*(P_R_{ % INDEX_0, % INDEX_1}*MOMENT...</td>\n",
       "      <td>&lt;T1&gt;e^4*MOMENTUM_23*MOMENTUM_24*(MOMENTUM_23+-...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1/2*i*e^2*(P_R_{ % INDEX_0, % INDEX_1}*MOMENT...</td>\n",
       "      <td>&lt;T1&gt;e^4*MOMENTUM_23*MOMENTUM_24*(MOMENTUM_23+-...</td>\n",
       "      <td>2-2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79782</th>\n",
       "      <td>-1/216*i*e^2*(e*sin(theta_W)/cos(theta_W)+3*e*...</td>\n",
       "      <td>&lt;T2&gt;1/17496*i*e^2*(e*sin(theta_W)/cos(theta_W)...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79811</th>\n",
       "      <td>1/8*i*e^2*(e*sin(theta_W)/cos(theta_W)+-e*cos(...</td>\n",
       "      <td>&lt;T1&gt;-1/12*i*e^2*(e*sin(theta_W)/cos(theta_W)+-...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79813</th>\n",
       "      <td>1/8*i*e^2*(e*sin(theta_W)/cos(theta_W)+-e*cos(...</td>\n",
       "      <td>&lt;T1&gt;-1/12*i*e^2*(e*sin(theta_W)/cos(theta_W)+-...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79854</th>\n",
       "      <td>1/54*i*e^2*(e*sin(theta_W)/cos(theta_W)+(-3)*e...</td>\n",
       "      <td>&lt;T2&gt;2/2187*i*e^2*(e*sin(theta_W)/cos(theta_W)+...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79856</th>\n",
       "      <td>1/54*i*e^2*(e*sin(theta_W)/cos(theta_W)+(-3)*e...</td>\n",
       "      <td>&lt;T2&gt;2/2187*i*e^2*(e*sin(theta_W)/cos(theta_W)+...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26017 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     amp  \\\n",
       "11     -1/2*i*e^2*(P_L_{ % INDEX_0, % INDEX_1}*MOMENT...   \n",
       "12     -1/2*i*e^2*(P_L_{ % INDEX_0, % INDEX_1}*MOMENT...   \n",
       "13     -1/2*i*e^2*(P_R_{ % INDEX_0, % INDEX_1}*MOMENT...   \n",
       "14     -1/2*i*e^2*(P_R_{ % INDEX_0, % INDEX_1}*MOMENT...   \n",
       "15     -1/2*i*e^2*(P_R_{ % INDEX_0, % INDEX_1}*MOMENT...   \n",
       "...                                                  ...   \n",
       "79782  -1/216*i*e^2*(e*sin(theta_W)/cos(theta_W)+3*e*...   \n",
       "79811  1/8*i*e^2*(e*sin(theta_W)/cos(theta_W)+-e*cos(...   \n",
       "79813  1/8*i*e^2*(e*sin(theta_W)/cos(theta_W)+-e*cos(...   \n",
       "79854  1/54*i*e^2*(e*sin(theta_W)/cos(theta_W)+(-3)*e...   \n",
       "79856  1/54*i*e^2*(e*sin(theta_W)/cos(theta_W)+(-3)*e...   \n",
       "\n",
       "                                                   sqamp process  length  \n",
       "11     <BOS>e^4*MOMENTUM_13*MOMENTUM_34*(MOMENTUM_12+...     2-2      24  \n",
       "12     <BOS>e^4*MOMENTUM_13*MOMENTUM_34*(MOMENTUM_12+...     2-2      24  \n",
       "13     <T1>e^4*MOMENTUM_23*MOMENTUM_24*(MOMENTUM_23+-...     2-2      25  \n",
       "14     <T1>e^4*MOMENTUM_23*MOMENTUM_24*(MOMENTUM_23+-...     2-2      25  \n",
       "15     <T1>e^4*MOMENTUM_23*MOMENTUM_24*(MOMENTUM_23+-...     2-2      25  \n",
       "...                                                  ...     ...     ...  \n",
       "79782  <T2>1/17496*i*e^2*(e*sin(theta_W)/cos(theta_W)...     2-3     797  \n",
       "79811  <T1>-1/12*i*e^2*(e*sin(theta_W)/cos(theta_W)+-...     2-3     800  \n",
       "79813  <T1>-1/12*i*e^2*(e*sin(theta_W)/cos(theta_W)+-...     2-3     800  \n",
       "79854  <T2>2/2187*i*e^2*(e*sin(theta_W)/cos(theta_W)+...     2-3     809  \n",
       "79856  <T2>2/2187*i*e^2*(e*sin(theta_W)/cos(theta_W)+...     2-3     809  \n",
       "\n",
       "[26017 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "660b7d20-3ffe-4ace-b01f-37746f9a02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_og = pd.read_csv(\"/pscratch/sd/r/ritesh11/SYMBA_arxiv/data/EW/EW_2-2-3test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "924e0151-3c47-45e3-b298-60c8ad0e5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_process_dict = dict(zip(df_test_og['amp'], df_test_og['process']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7911bd5d-1d01-4e18-8584-a0bab878168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_778009/338770338.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_corr_unique['process'] = df_corr_unique['amp'].map(amp_process_dict)\n",
      "/tmp/ipykernel_778009/338770338.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_unique['process'] = df_unique['amp'].map(amp_process_dict)\n"
     ]
    }
   ],
   "source": [
    "# Add process column to both dataframes using amp as key\n",
    "df_corr_unique['process'] = df_corr_unique['amp'].map(amp_process_dict)\n",
    "df_unique['process'] = df_unique['amp'].map(amp_process_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeea49b8-6c3b-4174-9a28-1637bcf4cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_22_corr = (df_corr_unique['process'] == \"2-2\").sum()\n",
    "count_23_corr = (df_corr_unique['process'] == \"2-3\").sum()\n",
    "count_24_corr = (df_corr_unique['process'] == \"2-4\").sum()\n",
    "\n",
    "count_22 = (df_unique['process'] == \"2-2\").sum()\n",
    "count_23 = (df_unique['process'] == \"2-3\").sum()\n",
    "count_24 = (df_unique['process'] == \"2-4\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c0da0f-6983-4eff-9c59-1daecb6440ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14f9586a-57ae-4c61-a1fd-2700e6c01c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.837037037037037)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_22_corr / count_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fd99079-419d-4470-911c-a33310676f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.83558942478024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_23_corr / count_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14d33a72-f051-4bdd-aa11-8c06ca1df5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0014064697609001407)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_24_corr / count_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5acb9b17-1ed4-4eb8-bac8-8c8c18a36c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50afcff6-9270-4d81-922a-798368670569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1433990895295903"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_preds / len(complete_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5680d-3484-43ba-83bf-11676f71c061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.6.0",
   "language": "python",
   "name": "pytorch-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
